\chapter{Grundlagen}\label{chapter:Grundlagen}
Mit dem Wachstum der Datenmengen setzen Unternehmen zunehmend auf \ac{LH}-Architekturen, um strukturierte und unstrukturierte Daten zu verwalten.\footcite[Vgl.][S. 1]{armbrustLakehouseNewGeneration2021} 
Weder \ac{DL} noch \ac{DW}-Systeme gelten als ideal für moderne Anwendungsfälle, insbesondere bei fortschrittlichen Analysen wie \ac{ML} Anwendungen, da führende \ac{ML}-Systeme schlecht auf \ac{DW}s aufbauen.\footcite[Vgl.][S. 5]{mazumdarDataLakehouseData2023} 
Im Gegensatz zu \ac{BI}-Abfragen, die kleine Datenmengen verarbeiten, benötigen \ac{ML}-Systeme große Datensätze und komplexen Code, der über \ac{SQL} hinausgeht.\footcite[Vgl.][S. 1]{armbrustLakehouseNewGeneration2021}  
Dies verdeutlicht die Herausforderungen der aktuellen Datenarchitekturen. 
Obwohl Cloud-basierte \ac{DL} und \ac{DW}-Lösungen durch die Trennung von Speicher und Rechenressourcen kosteneffizient wirken, führen sie zu erheblicher Komplexität.
Moderne Architekturen erfordern oft einen mehrstufigen \ac{ETL}-Prozess, bei dem Daten zunächst roh im \ac{DL} und anschließend im \ac{DW} gespeichert werden. Dieser Prozess ist zeitaufwendig, komplex und anfällig für Fehler. \ac{LH}-Architekturen lösen diese Probleme, indem sie offene Speicherformate (bspw. Delta Lake) mit Funktionen von \ac{DW}-Systemen kombinieren, wie \ac{ACID}-Transaktionen und Abfrageoptimierungen (bspw. DuckDB und Ibis).

\section{Entstehung des Data Lakehouse}
Traditionelle Datenbanken, sogenannte \ac{OLTP}-Systeme, wurden entwickelt, um tägliche Transaktionen effizient zu verarbeiten und schnellen sowie konsistenten Zugriff auf Daten zu gewährleisten.\footcite[Vgl.][45]{vaismanDataWarehouseSystems2014} 
\ac{OLTP}-Systeme sind somit optimiert für hohe Transaktionslasten und verwenden normalisierte Datenstrukturen, um Anomalien bei Updates zu vermeiden.\footcite[Vgl.][45 ff.]{vaismanDataWarehouseSystems2014} 
Diese starke Normalisierung macht sie jedoch ineffizient für komplexe Analysen, bei denen große Datenmengen verarbeitet oder mehrere Tabellen verknüpft werden müssen.\footcite[Vgl.][45 ff.]{vaismanDataWarehouseSystems2014}  
Aus diesem Grund wurden \ac{OLAP}-Systeme entwickelt, die auf Datenanalyse und Entscheidungsunterstützung ausgerichtet sind. \ac{OLAP}-Queries erfordern oft vollständige Tabellenscans und Aggregationen, wofür \ac{OLTP}-Systeme ungeeignet sind.\footcite[Vgl.][46]{vaismanDataWarehouseSystems2014} 

Datenbanken für analytische Zwecke, sogenannte \ac{DW}s, entstanden als zentrale Speicherorte für strukturierte Daten, die aus verschiedenen Quellen über \ac{ETL}-Prozesse integriert werden. Diese Daten werden häufig in relationalen Modellen wie Stern- oder Schneeflockenschemata organisiert, um eine effiziente Abfrage und Berichterstellung zu ermöglichen. \ac{DW}s sind ideal für Business Intelligence (BI) und historische Analysen, jedoch oft teuer und mit modernen Open-Source- oder Cloud-basierten Tools schwer kompatibel.

\ac{DL}s wurden als flexible Alternative zu \ac{DW}s entwickelt, um große Mengen an unstrukturierten, semi-strukturierten und strukturierten Daten zu speichern. Sie verzichten auf ein vorab definiertes Schema und speichern Daten in ihrer Rohform, was eine hohe Flexibilität bietet. Daten können nach Bedarf organisiert werden, beispielsweise in „Daten-Teiche“ (Data Ponds) für spezifische Datentypen wie rohe Daten, Anwendungsdaten oder Textdaten. Diese Strukturierung erleichtert die Verwaltung, aber \ac{DL}s stehen vor Herausforderungen wie mangelnder Datenqualität und der Gefahr von „Data Swamps“, in denen unorganisierte und schwer auffindbare Daten die Effektivität einschränken.

\section{Definition und Beschreibung von einem Data Lakehouse}
Um die Stärken von \ac{DW}s und \ac{DL}s zu vereinen, wurde die \ac{LH}-Architektur entwickelt. Sie kombiniert die skalierbare, flexible Speicherfähigkeit von \ac{DL}s mit den strukturierten und integrierten Analysefunktionen von \ac{DW}s. \ac{LH}-Systeme unterstützen fortschrittliche Analysen und \ac{ML}-Anwendungen und nutzen offene Speicherformate wie Apache Parquet oder Delta Lake sowie Cloud-Objektspeicher wie Minio. Metadatenkataloge wie Apache Iceberg ermöglichen die Verwaltung von Daten und gewährleisten Konsistenz durch ACID-Transaktionen. Performance-Optimierungen wie Indexerstellung, Statistikpflege und effiziente Datenlayouts verbessern zusätzlich die Abfragegeschwindigkeit. Durch ihre Flexibilität, Skalierbarkeit und analytischen Fähigkeiten setzt sich die LH-Architektur zunehmend als Standard in der Datenverarbeitung durch.
