@book{kimball2013data,
  title={The data warehouse toolkit: The definitive guide to dimensional modeling},
  author={Kimball, Ralph and Ross, Margy},
  year={2013},
  publisher={John Wiley \& Sons}
}

@book{linstedt2015building,
  title={Building a scalable data warehouse with data vault 2.0},
  author={Linstedt, Daniel and Olschimke, Michael},
  year={2015},
  publisher={Morgan Kaufmann}
}

@article{russom2007bi,
  title={BI search and text analytics},
  author={Russom, Philip},
  journal={TDWI Best Practices Report},
  pages={9--11},
  year={2007}
}

@inproceedings{nambiar2006making,
  title={The Making of TPC-DS.},
  author={Nambiar, Raghunath Othayoth and Poess, Meikel},
  booktitle={VLDB},
  volume={6},
  pages={1049--1058},
  year={2006}
}

@online{nikolaParquetFileFormat2023b,
  author       = {Nikola, },
  title        = {Parquet file format - everything you need to know!},
  organization = {Data Mozart},
  url          = {https://data-mozart.com/parquet-file-format-everything-you-need-to-know/},
  urldate      = {2024-12-10},
  langid       = {english},
}

@online{chandraZOrderIndexingEfficient20234,
  author       = {Borók-Nagy, Zoltán and Luksa, Norbert},
  title        = {Speeding up Queries With Z-Order},
  organization = {Cloudera Blog},
  url          = {https://blog.cloudera.com/speeding-up-queries-with-z-order/},
  urldate      = {2024-12-10},
  langid       = {english},
}

@online{Dockerinc.DevelopFasterRun2025,
  author       = {{Docker Inc.}},
  title        = {Develop faster. Run anywhere.},
  organization = {Docker Inc.},
  url          = {https://www.docker.com/},
  urldate      = {2025-01-19},
  langid       = {german},
}

@online{Podman.DevelopFasterRun2025,
  author       = {Podman},
  title        = {The best free \& open source container tools.},
  organization = {Red Hat},
  url          = {https://podman.io/},
  urldate      = {2025-01-19},
  langid       = {german},
}

@online{horganBuildingTreeStructuredParzen2023,
  author       = {Horgan, Colin},
  title        = {Building a Tree-Structured Parzen Estimator from Scratch},
  organization = {Medium},
  url          = {https://towardsdatascience.com/building-a-tree-structured-parzen-estimator-from-scratch-kind-of-20ed31770478},
  urldate      = {2024-12-10},
  langid       = {english},
}

@online{Iceberg101Guide,
  author       = {Levy, Eran},
  title        = {Building a Tree-Structured Parzen Estimator from Scratch},
  organization = {Upsolver},
  url          = {https://www.upsolver.com/blog/iceberg-partitioning},
  urldate      = {2024-12-10},
  langid       = {english},
}

@article{10.1109/69.908985,
  title = {Analysis of the Clustering Properties of the Hilbert Space-Filling Curve},
  author = {Moon, Bongki and v. Jagadish, H. and Faloutsos, Christos and Saltz, Joel H.},
  year = {2001},
  month = jan,
  journal = {IEEE Trans. on Knowl. and Data Eng.},
  volume = {13},
  number = {1},
  pages = {124--141},
  publisher = {IEEE Educational Activities Department},
  address = {USA},
  issn = {1041-4347},
  doi = {10.1109/69.908985},
  abstract = {Several schemes for the linear mapping of a multidimensional space have been proposed for various applications, such as access methods for spatio-temporal databases and image compression. In these applications, one of the most desired properties from such linear mappings is clustering, which means the locality between objects in the multidimensional space being preserved in the linear space. It is widely believed that the Hilbert space-filling curve achieves the best clustering [1], [14]. In this paper, we analyze the clustering property of the Hilbert space-filling curve by deriving closed-form formulas for the number of clusters in a given query region of an arbitrary shape (e.g., polygons and polyhedra). Both the asymptotic solution for the general case and the exact solution for a special case generalize previous work [14]. They agree with the empirical results that the number of clusters depends on the hypersurface area of the query region and not on its hypervolume. We also show that the Hilbert curve achieves better clustering than the z curve. From a practical point of view, the formulas given in this paper provide a simple measure that can be used to predict the required disk access behaviors and, hence, the total access time.},
  issue_date = {January 2001},
  keywords = {data clustering,fractals.,Hilbert curve,Locality-preserving linear mapping,multiattribute access methods,range queries,space-filling curves},
  file = {/Users/david/Zotero/storage/29FBIW7X/Moon et al. - Analysis of the Clustering Properties of Hilbert S.pdf}
}

@inproceedings{10.1145/2464576.2501592,
  title = {An Evaluation of Sequential Model-Based Optimization for Expensive Blackbox Functions},
  booktitle = {Proceedings of the 15th Annual Conference Companion on Genetic and Evolutionary Computation},
  author = {Hutter, Frank and Hoos, Holger and {Leyton-Brown}, Kevin},
  year = {2013},
  series = {{{GECCO}} '13 Companion},
  pages = {1209--1216},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2464576.2501592},
  abstract = {We benchmark a sequential model-based optimization procedure, SMAC-BBOB, on the BBOB set of blackbox functions. We demonstrate that with a small budget of 10xD evaluations of D-dimensional functions, SMAC-BBOB in most cases outperforms the state-of-the-art blackbox optimizer CMA-ES. However, CMA-ES benefits more from growing the budget to 100xD, and for larger number of function evaluations SMAC-BBOB also requires increasingly large computational resources for building and using its models.},
  isbn = {978-1-4503-1964-5},
  keywords = {benchmarking,black-box optimization}
}

@inproceedings{10.1145/2588555.2610515,
  title = {Fine-Grained Partitioning for Aggressive Data Skipping},
  booktitle = {Proceedings of the 2014 {{ACM SIGMOD}} International Conference on Management of Data},
  author = {Sun, Liwen and Franklin, Michael J. and Krishnan, Sanjay and Xin, Reynold S.},
  year = {2014},
  series = {Sigmod '14},
  pages = {1115--1126},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2588555.2610515},
  abstract = {Modern query engines are increasingly being required to process enormous datasets in near real-time. While much can be done to speed up the data access, a promising technique is to reduce the need to access data through data skipping. By maintaining some metadata for each block of tuples, a query may skip a data block if the metadata indicates that the block does not contain relevant data. The effectiveness of data skipping, however, depends on how well the blocking scheme matches the query filters.In this paper, we propose a fine-grained blocking technique that reorganizes the data tuples into blocks with a goal of enabling queries to skip blocks aggressively. We first extract representative filters in a workload as features using frequent itemset mining. Based on these features, each data tuple can be represented as a feature vector. We then formulate the blocking problem as a optimization problem on the feature vectors, called Balanced MaxSkip Partitioning, which we prove is NP-hard. To find an approximate solution efficiently, we adopt the bottom-up clustering framework. We prototyped our blocking techniques on Shark, an open-source data warehouse system. Our experiments on TPC-H and a real-world workload show that our blocking technique leads to 2-5x improvement in query response time over traditional range-based blocking techniques.},
  isbn = {978-1-4503-2376-5},
  keywords = {algorithms,data warehouse,partitioning,query processing}
}

@inproceedings{10.1145/3211954.3211956,
  title = {{{GridFormation}}: {{Towards}} Self-Driven Online Data Partitioning Using Reinforcement Learning},
  booktitle = {Proceedings of the First International Workshop on Exploiting Artificial Intelligence Techniques for Data Management},
  author = {Durand, Gabriel Campero and Pinnecke, Marcus and Piriyev, Rufat and Mohsen, Mahmoud and Broneske, David and Saake, Gunter and Sekeran, Maya S. and Rodriguez, Fabi{\'a}n and Balami, Laxmi},
  year = {2018},
  series = {{{aiDM}}'18},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3211954.3211956},
  abstract = {In this paper we define a research agenda to develop a general framework supporting online autonomous tuning of data partitioning and layouts with a reinforcement learning formulation. We establish the core elements of our approach: agent, environment, action space and supporting components. Externally predicted workloads and the current physical design serve as input to our agent. The environment guides the search process by generating immediate rewards based on fresh cost estimates, for either the entirety or a sample of queries from the workload, and by deciding the possible actions given a state. This set of actions is configurable, enabling the representation of different partitioning problems. For use in an online setting the agent learns a fixed-length sequence of n actions that maximize the temporal reward for the predicted workload. Through an initial implementation we assert the feasibility of our approach. To conclude, we list open challenges for this work.},
  articleno = {1},
  isbn = {978-1-4503-5851-4},
  keywords = {Adaptive layouts,Deep Q-Learning,Physical design}
}

@inproceedings{10.1145/3318464.3389704,
  title = {Learning a Partitioning Advisor for Cloud Databases},
  booktitle = {Proceedings of the 2020 {{ACM SIGMOD}} International Conference on Management of Data},
  author = {Hilprecht, Benjamin and Binnig, Carsten and R{\"o}hm, Uwe},
  year = {2020},
  series = {Sigmod '20},
  pages = {143--157},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3318464.3389704},
  abstract = {Cloud vendors provide ready-to-use distributed DBMS solutions as a service. While the provisioning of a DBMS is usually fully automated, customers typically still have to make important design decisions which were traditionally made by the database administrator such as finding an optimal partitioning scheme for a given database schema and workload. In this paper, we introduce a new learned partitioning advisor based on Deep Reinforcement Learning (DRL) for OLAP-style workloads. The main idea is that a DRL agent learns the cost tradeoffs of different partitioning schemes and can thus automate the partitioning decision. In the evaluation, we show that our advisor is able to find non-trivial partitionings for a wide range of workloads and outperforms more classical approaches for automated partitioning design.},
  isbn = {978-1-4503-6735-6},
  keywords = {database management systems,database tuning,machine learning}
}

@inproceedings{10.1145/3377930.3389817,
  title = {Multiobjective Tree-Structured Parzen Estimator for Computationally Expensive Optimization Problems},
  booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference},
  author = {Ozaki, Yoshihiko and Tanigaki, Yuki and Watanabe, Shuhei and Onishi, Masaki},
  year = {2020},
  series = {Gecco '20},
  pages = {533--541},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3377930.3389817},
  abstract = {Practitioners often encounter computationally expensive multiobjective optimization problems to be solved in a variety of real-world applications. On the purpose of challenging these problems, we propose a new surrogate-based multiobjective optimization algorithm that does not require a large evaluation budget. It is called Multiobjective Tree-structured Parzen Estimator (MOTPE) and is an extension of the tree-structured Parzen estimator widely used to solve expensive single-objective optimization problems. Our empirical evidences reveal that MOTPE can approximate Pareto fronts of many benchmark problems better than existing methods with a limited budget. In this paper, we discuss furthermore the influence of MOTPE configurations to understand its behavior.},
  isbn = {978-1-4503-7128-5},
  keywords = {bayesian optimization,computationally expensive optimization,infill criteria,machine learning,multiobjective optimization,surrogate modeling,tree-structured parzen estimator}
}

@inproceedings{10.1145/3626246.3653379,
  title = {Automated Multidimensional Data Layouts in Amazon Redshift},
  booktitle = {Companion of the 2024 International Conference on Management of Data},
  author = {Ding, Jialin and Abrams, Matt and Bandyopadhyay, Sanghita and Di Palma, Luciano and Ji, Yanzhu and Pagano, Davide and Paliwal, Gopal and Parchas, Panos and Pfeil, Pascal and Polychroniou, Orestis and Saxena, Gaurav and Shah, Aamer and Voloder, Amina and Xiao, Sherry and Zhang, Davis and Kraska, Tim},
  year = {2024},
  series = {Sigmod/Pods '24},
  pages = {55--67},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3626246.3653379},
  abstract = {Analytic data systems typically use data layouts to improve the performance of scanning and filtering data. Common data layout techniques include single-column sort keys, compound sort keys, and more complex multidimensional data layouts such as the Z-order. An appropriately-selected data layout over a table, in combination with metadata such as zone maps, enables the system to skip irrelevant data blocks when scanning the table, which reduces the amount of data scanned and improves query performance.In this paper, we introduce Multidimensional Data Layouts (MDDL), a new data layout technique which outperforms existing data layout techniques for query workloads with repetitive scan filters. Unlike existing data layout approaches, which typically sort tables based on columns, MDDL sorts tables based on a collection of predicates, which enables a much higher degree of specialization to the user's workload. We additionally introduce an algorithm for automatically learning the best MDDL for each table based on telemetry collected from the historical workload. We implemented MDDL within Amazon Redshift. Benchmarks on internal datasets and workloads show that MDDL achieves up to 85\% reduction in end-to-end workload runtime compared to using traditional column-based data layout techniques. MDDL is, to the best of our knowledge, the first data layout technique in a commercial product that sorts based on predicates and automatically learns the best predicates.},
  isbn = {9798400704222},
  keywords = {analytic database,data warehouse,machine learning,sort key},
  file = {/Users/david/Zotero/storage/ABNX3FAQ/Ding et al. - 2024 - Automated Multidimensional Data Layouts in Amazon .pdf}
}

@inproceedings{10.1145/375663.375703,
  title = {Materialized View Selection and Maintenance Using Multi-Query Optimization},
  booktitle = {Proceedings of the 2001 {{ACM SIGMOD}} International Conference on Management of Data},
  author = {Mistry, Hoshi and Roy, Prasan and Sudarshan, S. and Ramamritham, Krithi},
  year = {2001},
  series = {Sigmod '01},
  pages = {307--318},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/375663.375703},
  abstract = {Materialized views have been found to be very effective at speeding up queries, and are increasingly being supported by commercial databases and data warehouse systems. However, whereas the amount of data entering a warehouse and the number of materialized views are rapidly increasing, the time window available for maintaining materialized views is shrinking. These trends necessitate efficient techniques for the maintenance of materialized views.In this paper, we show how to find an efficient plan for the maintenance of a set of materialized views, by exploiting common subexpressions between different view maintenance expressions. In particular, we show how to efficiently select (a) expressions and indices that can be effectively shared, by transient materialization; (b) additional expressions and indices for permanent materialization; and (c) the best maintenance plan --- incremental or recomputation --- for each view. These three decisions are highly interdependent, and the choice of one affects the choice of the others. We develop a framework that cleanly integrates the various choices in a systematic and efficient manner. Our evaluations show that many-fold improvement in view maintenance time can be achieved using our techniques. Our algorithms can also be used to efficiently select materialized views to speed up workloads containing queries and updates.},
  isbn = {1-58113-332-4},
  file = {/Users/david/Zotero/storage/SA2A9Y42/Mistry et al. - 2001 - Materialized view selection and maintenance using .pdf}
}

@article{10.14778/1687553.1687625,
  title = {Column-Oriented Database Systems},
  author = {Abadi, Daniel and Boncz, Peter and Harizopoulos, Stavros},
  year = {2009},
  month = aug,
  journal = {Proc. VLDB Endow.},
  volume = {2},
  number = {2},
  pages = {1664--1665},
  publisher = {VLDB Endowment},
  issn = {2150-8097},
  doi = {10.14778/1687553.1687625},
  abstract = {Column-oriented database systems (column-stores) have attracted a lot of attention in the past few years. Column-stores, in a nutshell, store each database table column separately, with attribute values belonging to the same column stored contiguously, compressed, and densely packed, as opposed to traditional database systems that store entire records (rows) one after the other. Reading a subset of a table's columns becomes faster, at the potential expense of excessive disk-head seeking from column to column for scattered reads or updates. After several dozens of research papers and at least a dozen of new column-store start-ups, several questions remain. Are these a new breed of systems or simply old wine in new bottles? How easily can a major row-based system achieve column-store performance? Are column-stores the answer to effortlessly support large-scale data-intensive applications? What are the new, exciting system research problems to tackle? What are the new applications that can be potentially enabled by column-stores? In this tutorial, we present an overview of column-oriented database system technology and address these and other related questions.},
  issue_date = {August 2009}
}

@article{10.14778/2733004.2733009,
  title = {{{TPC-DI}}: The First Industry Benchmark for Data Integration},
  author = {Poess, Meikel and Rabl, Tilmann and Jacobsen, Hans-Arno and Caufield, Brian},
  year = {2014},
  month = aug,
  journal = {Proc. VLDB Endow.},
  volume = {7},
  number = {13},
  pages = {1367--1378},
  publisher = {VLDB Endowment},
  issn = {2150-8097},
  doi = {10.14778/2733004.2733009},
  abstract = {Historically, the process of synchronizing a decision support system with data from operational systems has been referred to as Extract, Transform, Load (ETL) and the tools supporting such process have been referred to as ETL tools. Recently, ETL was replaced by the more comprehensive acronym, data integration (DI). DI describes the process of extracting and combining data from a variety of data source formats, transforming that data into a unified data model representation and loading it into a data store. This is done in the context of a variety of scenarios, such as data acquisition for business intelligence, analytics and data warehousing, but also synchronization of data between operational applications, data migrations and conversions, master data management, enterprise data sharing and delivery of data services in a service-oriented architecture context, amongst others. With these scenarios relying on up-to-date information it is critical to implement a highly performing, scalable and easy to maintain data integration system. This is especially important as the complexity, variety and volume of data is constantly increasing and performance of data integration systems is becoming very critical. Despite the significance of having a highly performing DI system, there has been no industry standard for measuring and comparing their performance. The TPC, acknowledging this void, has released TPC-DI, an innovative benchmark for data integration. This paper motivates the reasons behind its development, describes its main characteristics including workload, run rules, metric, and explains key decisions.},
  issue_date = {August 2014}
}

@article{10.14778/3358701.3358707,
  title = {Optimal Column Layout for Hybrid Workloads},
  author = {Athanassoulis, Manos and B{\o}gh, Kenneth S. and Idreos, Stratos},
  year = {2019},
  month = sep,
  journal = {Proc. VLDB Endow.},
  volume = {12},
  number = {13},
  pages = {2393--2407},
  publisher = {VLDB Endowment},
  issn = {2150-8097},
  doi = {10.14778/3358701.3358707},
  abstract = {Data-intensive analytical applications need to support both efficient reads and writes. However, what is usually a good data layout for an update-heavy workload, is not well-suited for a read-mostly one and vice versa. Modern analytical data systems rely on columnar layouts and employ delta stores to inject new data and updates.We show that for hybrid workloads we can achieve close to one order of magnitude better performance by tailoring the column layout design to the data and query workload. Our approach navigates the possible design space of the physical layout: it organizes each column's data by determining the number of partitions, their corresponding sizes and ranges, and the amount of buffer space and how it is allocated. We frame these design decisions as an optimization problem that, given workload knowledge and performance requirements, provides an optimal physical layout for the workload at hand. To evaluate this work, we build an in-memory storage engine, Casper, and we show that it outperforms state-of-the-art data layouts of analytical systems for hybrid workloads. Casper delivers up to 2.32x higher throughput for update-intensive workloads and up to 2.14x higher throughput for hybrid workloads. We further show how to make data layout decisions robust to workload variation by carefully selecting the input of the optimization.},
  issue_date = {September 2019}
}

@article{10.14778/3407790.3407834,
  title = {Fast and Effective Distribution-Key Recommendation for Amazon Redshift},
  author = {Parchas, Panos and Naamad, Yonatan and Van Bouwel, Peter and Faloutsos, Christos and Petropoulos, Michalis},
  year = {2020},
  month = jul,
  journal = {Proc. VLDB Endow.},
  volume = {13},
  number = {12},
  pages = {2411--2423},
  publisher = {VLDB Endowment},
  issn = {2150-8097},
  doi = {10.14778/3407790.3407834},
  abstract = {How should we split data among the nodes of a distributed data warehouse in order to boost performance for a forecasted workload? In this paper, we study the effect of different data partitioning schemes on the overall network cost of pairwise joins. We describe a generally-applicable data distribution framework initially designed for Amazon Redshift, a fully-managed petabyte-scale data warehouse in the cloud. To formalize the problem, we first introduce the Join Multi-Graph, a concise graph-theoretic representation of the workload history of a cluster. We then formulate the "Distribution-Key Recommendation" problem - a novel combinatorial problem on the Join Multi-Graph - and relate it to problems studied in other subfields of computer science. Our theoretical analysis proves that "Distribution-Key Recommendation" is NP-complete and is hard to approximate efficiently. Thus, we propose BaW, a hybrid approach that combines heuristic and exact algorithms to find a good data distribution scheme. Our extensive experimental evaluation on real and synthetic data showcases the efficacy of our method into recommending optimal (or close to optimal) distribution keys, which improve the cluster performance by reducing network cost up to 32x in some real workloads.},
  issue_date = {August 2020}
}

@inproceedings{10.5555/1083592.1083658,
  title = {C-Store: A Column-Oriented {{DBMS}}},
  booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
  author = {Stonebraker, Mike and Abadi, Daniel J. and Batkin, Adam and Chen, Xuedong and Cherniack, Mitch and Ferreira, Miguel and Lau, Edmond and Lin, Amerson and Madden, Sam and O'Neil, Elizabeth and O'Neil, Pat and Rasin, Alex and Tran, Nga and Zdonik, Stan},
  year = {2005},
  series = {Vldb '05},
  pages = {553--564},
  publisher = {VLDB Endowment},
  address = {Trondheim, Norway},
  abstract = {This paper presents the design of a read-optimized relational DBMS that contrasts sharply with most current systems, which are write-optimized. Among the many differences in its design are: storage of data by column rather than by row, careful coding and packing of objects into storage including main memory during query processing, storing an overlapping collection of column-oriented projections, rather than the current fare of tables and indexes, a non-traditional implementation of transactions which includes high availability and snapshot isolation for read-only transactions, and the extensive use of bitmap indexes to complement B-tree structures.We present preliminary performance data on a subset of TPC-H and show that the system we are building, C-Store, is substantially faster than popular commercial products. Hence, the architecture looks very encouraging.},
  isbn = {1-59593-154-6},
  file = {/Users/david/Zotero/storage/4WQS9H72/Stonebraker et al. - 2005 - C-store a column-oriented DBMS.pdf}
}

@article{10.5555/1152682.1152691,
  title = {Is {{Morton}} Layout Competitive for Large Two-Dimensional Arrays yet? {{Research Articles}}},
  author = {Thiyagalingam, Jeyarajan and Beckmann, Olav and Kelly, Paul H. J.},
  year = {2006},
  month = sep,
  journal = {Concurr. Comput.\,: Pract. Exper.},
  volume = {18},
  number = {11},
  pages = {1509--1539},
  publisher = {{John Wiley and Sons Ltd.}},
  address = {GBR},
  issn = {1532-0626},
  abstract = {Two-dimensional arrays are generally arranged in memory in row-major order or column-major order. Traversing a row-major array in column-major order, or vice versa, leads to poor spatial locality. With large arrays the performance loss can be a factor of 10 or more. This paper explores the Morton storage layout, which has substantial spatial locality whether traversed in row-major or column-major order. Using a small suite of dense kernels working on two-dimensional arrays, we have carried out an extensive study of the impact of poor array layout and of whether Morton layout can offer an attractive compromise. We show that Morton layout can lead to better performance than the worse of the two canonical layouts; however, the performance of Morton layout compared to the better choice of canonical layout is often disappointing. We further study one simple improvement of the basic Morton scheme: we show that choosing the correct alignment for the base address of an array in Morton layout can sometimes significantly improve the competitiveness of this layout. Copyright {\copyright} 2006 John Wiley \& Sons, Ltd.},
  issue_date = {September 2006},
  keywords = {compilers,memory organization,Morton order,space-filling curves,spatial locality,tiling}
}

@article{10.5555/13900.18159,
  title = {A Measure of Transaction Processing Power},
  author = {Bitton, Dina and Brown, Mark and Catell, Rick and Ceri, Stefano and Chou, Tim and DeWitt, Dave and Gawlick, Dieter and {Garcia-Molina}, Hector and Good, Bob and Gray, Jim and Homan, Pete and Jolls, Bob and Lukes, Tony and Lazowska, Ed and Nauman, John and Pong, Mike and Spector, Alfred and Trieber, Kent and Sammer, Harald and Serlin, Omri and Stonebraker, Mike and Reuter, Andreas and Weinberger, Peter},
  year = {1985},
  month = apr,
  journal = {Datamation},
  volume = {31},
  number = {7},
  pages = {112--118},
  publisher = {Cahners Publishing Company},
  address = {USA},
  issn = {0011-6963},
  issue_date = {April 1, 1985},
  file = {/Users/david/Zotero/storage/4DXQYXZK/AMeasureOfTransactionProcessingPower.pdf}
}

@book{10.5555/2371241,
  title = {Scientific Research in Information Systems: A Beginner's Guide},
  author = {Recker, Jan},
  year = {2012},
  publisher = {Springer Publishing Company, Incorporated},
  abstract = {This book is designed to introduce doctoral and other higher-degree research students to the process of scientific research in the fields of Information Systems as well as fields of Information Technology, Business Process Management and other related disciplines within the social sciences. It guides research students in their process of learning the life of a researcher. In doing so, it provides an understanding of the essential elements, concepts and challenges of the journey into research studies. It also provides a gateway for the student to inquire deeper about each element covered. Comprehensive and broad but also succinct and compact, the book is focusing on the key principles and challenges for a novice doctoral student.},
  isbn = {3-642-30047-2}
}

@book{10.5555/3285267,
  title = {A Tutorial on Thompson Sampling},
  author = {Russo, Daniel J. and Roy, Benjamin Van and Kazerouni, Abbas and Osband, Ian and Wen, Zheng},
  year = {2018},
  publisher = {Now Publishers Inc.},
  address = {Hanover, MA, USA},
  abstract = {Thompson sampling is an algorithm for online decision problems where actions are taken sequentially in a manner that must balance between exploiting what is known to maximize immediate performance and investing to accumulate new information that may improve future performance. The algorithm addresses a broad range of problems in a computationally efficient manner and is therefore enjoying wide use. A Tutorial on Thompson Sampling covers the algorithm and its application, illustrating concepts through a range of examples, including Bernoulli bandit problems, shortest path problems, product recommendation, assortment, active learning with neural networks, and reinforcement learning in Markov decision processes. Most of these problems involve complex information structures, where information revealed by taking an action informs beliefs about other actions. It also discusses when and why Thompson sampling is or is not effective and relations to alternative algorithms.},
  isbn = {1-68083-470-3},
  file = {/Users/david/Zotero/storage/IQQYQ6A6/Russo et al. - 2020 - A Tutorial on Thompson Sampling.pdf}
}

@book{10.5555/531075,
  title = {Adaptation in Natural and Artificial Systems: {{An}} Introductory Analysis with Applications to Biology, Control and Artificial Intelligence},
  author = {Holland, John H.},
  year = {1992},
  publisher = {MIT Press},
  address = {Cambridge, MA, USA},
  abstract = {From the Publisher:Genetic algorithms are playing an increasingly important role in studies of complex adaptive systems, ranging from adaptive agents in economic theory to the use of machine learning techniques in the design of complex devices such as aircraft turbines and integrated circuits. Adaptation in Natural and Artificial Systems is the book that initiated this field of study, presenting the theoretical foundations and exploring applications. In its most familiar form, adaptation is a biological process, whereby organisms evolve by rearranging genetic material to survive in environments confronting them. In this now classic work, Holland presents a mathematical model that allows for the nonlinearity of such complex interactions. He demonstrates the model's universality by applying it to economics, physiological psychology, game theory, and artificial intelligence and then outlines the way in which this approach modifies the traditional views of mathematical genetics. Initially applying his concepts to simply defined artificial systems with limited numbers of parameters, Holland goes on to explore their use in the study of a wide range of complex, naturally occuring processes, concentrating on systems having multiple factors that interact in nonlinear ways. Along the way he accounts for major effects of coadaptation and coevolution: the emergence of building blocks, or schemata, that are recombined and passed on to succeeding generations to provide, innovations and improvements. John H. Holland is Professor of Psychology and Professor of Electrical Engineering and Computer Science at the University of Michigan. He is also Maxwell Professor at the Santa Fe Institute and isDirector of the University of Michigan/Santa Fe Institute Advanced Research Program.},
  isbn = {0-262-08213-6},
  file = {/Users/david/Zotero/storage/MDALKG5U/Holland - 1992 - Adaptation in natural and artificial systems An i.pdf}
}

@inproceedings{10.5555/645924.671173,
  title = {Small Materialized Aggregates: A Light Weight Index Structure for Data Warehousing},
  booktitle = {Proceedings of the 24rd International Conference on Very Large Data Bases},
  author = {Moerkotte, Guido},
  year = {1998},
  series = {Vldb '98},
  pages = {476--487},
  publisher = {Morgan Kaufmann Publishers Inc.},
  address = {San Francisco, CA, USA},
  isbn = {1-55860-566-5}
}

@inproceedings{10.5555/645926.671701,
  title = {Automated Selection of Materialized Views and Indexes in {{SQL}} Databases},
  booktitle = {Proceedings of the 26th International Conference on Very Large Data Bases},
  author = {Agrawal, Sanjay and Chaudhuri, Surajit and Narasayya},
  year = {2000},
  series = {Vldb '00},
  pages = {496--505},
  publisher = {Morgan Kaufmann Publishers Inc.},
  address = {San Francisco, CA, USA},
  isbn = {1-55860-715-3}
}

@inproceedings{10020719,
  title = {From Data Warehouse to Lakehouse: A Comparative Review},
  booktitle = {2022 {{IEEE}} International Conference on Big Data (Big Data)},
  author = {Harby, Ahmed A. and Zulkernine, Farhana},
  year = {2022},
  pages = {389--395},
  doi = {10.1109/BigData55660.2022.10020719},
  keywords = {Big data,Big Data applications,Data Lake,Data Lakehouse,Data mining,Data Warehouse,Data warehouses,Distributed databases,Information systems},
  file = {/Users/david/Zotero/storage/6TZRMP2H/Harby and Zulkernine - 2022 - From data warehouse to lakehouse a comparative re.pdf}
}

@inproceedings{4721476,
  title = {A Database Engine for Flexible Real-Time Available-to-Promise},
  booktitle = {2008 {{IEEE}} Symposium on Advanced Management of Information for Globalized Enterprises ({{AMIGE}})},
  author = {Tinnefeld, Christian and Krueger, Jens and Schaffner, Jan and Bog, Anja},
  year = {2008},
  pages = {1--5},
  doi = {10.1109/AMIGE.2008.ECP.34},
  keywords = {Concurrency control,Data warehouses,Database systems,Delay,Hardware,Indexing,Multithreading,Real time systems,Relational databases,Search engines}
}

@inproceedings{9377740,
  title = {Extensible Data Skipping},
  booktitle = {2020 {{IEEE}} International Conference on Big Data (Big Data)},
  author = {{Ta-Shma}, Paula and Khazma, Guy and Lushi, Gal and Feder, Oshrit},
  year = {2020},
  pages = {372--382},
  doi = {10.1109/BigData50022.2020.9377740},
  keywords = {big data,Big Data,cloud computing,Geospatial analysis,Indexes,indexing methods,Libraries,Metadata,query processing,Servers,Sparks}
}

@inproceedings{agrawalIntegratingVerticalHorizontal2004,
  title = {Integrating Vertical and Horizontal Partitioning into Automated Physical Database Design},
  booktitle = {Proceedings of the 2004 {{ACM SIGMOD}} International Conference on Management of Data},
  author = {Agrawal, Sanjay and Yang, Beverly and Narasayya},
  year = {2004},
  series = {Sigmod '04},
  pages = {359--370},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1007568.1007609},
  abstract = {In addition to indexes and materialized views, horizontal and vertical partitioning are important aspects of physical design in a relational database system that significantly impact performance. Horizontal partitioning also provides manageability; database administrators often require indexes and their underlying tables partitioned identically so as to make common operations such as backup/restore easier. While partitioning is important, incorporating partitioning makes the problem of automating physical design much harder since: (a) The choices of partitioning can strongly interact with choices of indexes and materialized views. (b) A large new space of physical design alternatives must be considered. (c) Manageability requirements impose a new constraint on the problem. In this paper, we present novel techniques for designing a scalable solution to this integrated physical design problem that takes both performance and manageability into account. We have implemented our techniques and evaluated it on Microsoft SQL Server. Our experiments highlight: (a) the importance of taking an integrated approach to automated physical design and (b) the scalability of our techniques.},
  isbn = {1-58113-859-8}
}

@article{armbrustLakehouseNewGeneration2021,
  title = {Lakehouse: {{A New Generation}} of {{Open Platforms}} That {{Unify Data Warehousing}} and {{Advanced Analytics}}},
  author = {Armbrust, Michael and Ghodsi, Ali and Xin, Reynold and Zaharia, Matei},
  year = {2021},
  abstract = {This paper argues that the data warehouse architecture as we know it today will wither in the coming years and be replaced by a new architectural pattern, the Lakehouse, which will (i) be based on open direct-access data formats, such as Apache Parquet, (ii) have firstclass support for machine learning and data science, and (iii) offer state-of-the-art performance. Lakehouses can help address several major challenges with data warehouses, including data staleness, reliability, total cost of ownership, data lock-in, and limited use-case support. We discuss how the industry is already moving toward Lakehouses and how this shift may affect work in data management. We also report results from a Lakehouse system using Parquet that is competitive with popular cloud data warehouses on TPC-DS.},
  langid = {english},
  file = {/Users/david/Zotero/storage/6FDML864/Armbrust et al. - 2021 - Lakehouse A New Generation of Open Platforms that.pdf}
}

@article{article,
  title = {A Novel Design Methodology for a Multioctave {{GaN-HEMT}} Power Amplifier Using Clustering Guided Bayesian Optimization},
  author = {Guo, Jia and Crupi, Giovanni and Cai, Jialin},
  year = {2022},
  month = jan,
  journal = {IEEE access : practical innovations, open solutions},
  volume = {10},
  pages = {1--1},
  doi = {10.1109/ACCESS.2022.3175870},
  file = {/Users/david/Zotero/storage/977AXQMP/Guo et al. - 2022 - A Novel Design Methodology for a Multioctave GaN-H.pdf}
}

@inproceedings{bergstraMakingScienceModel2013,
  title = {Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures},
  booktitle = {Proceedings of the 30th {{International Conference}} on {{International Conference}} on {{Machine Learning}} - {{Volume}} 28},
  author = {Bergstra and Yamins. and Cox},
  year = {2013},
  pages = {I--115--I--123},
  publisher = {JMLR.org},
  address = {Atlanta, GA, USA},
  file = {/Users/david/Zotero/storage/VQF23D2G/Bergstra et al. - 2012 - Making a Science of Model Search.pdf}
}

@article{bergstraRandomSearchHyperparameter2012,
  title = {Random Search for Hyper-Parameter Optimization},
  author = {Bergstra and Bengio},
  year = {2012},
  journal = {J. Mach. Learn. Res.},
  volume = {13},
  number = {null},
  pages = {281--305},
  publisher = {JMLR.org},
  isbn = {1532-4435},
  keywords = {deep learning global optimization model selection neural networks response surface modeling},
  file = {/Users/david/Zotero/storage/C2GVDTFI/Bergstra and Bengio - Random Search for Hyper-Parameter Optimization.pdf}
}

@article{breimanRandomForests2001,
  title = {Random {{Forests}}},
  author = {Breiman, Leo},
  year = {2001},
  month = oct,
  journal = {Machine Learning},
  volume = {45},
  number = {1},
  pages = {5--32},
  issn = {1573-0565},
  doi = {10.1023/A:1010933404324},
  abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148--156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.}
}

@article{breretonLessonsApplyingSystematic2007,
  title = {Lessons from Applying the Systematic Literature Review Process within the Software Engineering Domain},
  author = {Brereton, Pearl and Kitchenham, Barbara A. and Budgen, David and Turner, Mark and Khalil, Mohamed},
  year = {2007},
  month = apr,
  journal = {Software Performance},
  volume = {80},
  number = {4},
  pages = {571--583},
  issn = {0164-1212},
  doi = {10.1016/j.jss.2006.07.009},
  abstract = {A consequence of the growing number of empirical studies in software engineering is the need to adopt systematic approaches to assessing and aggregating research outcomes in order to provide a balanced and objective summary of research evidence for a particular topic. The paper reports experiences with applying one such approach, the practice of systematic literature review, to the published studies relevant to topics within the software engineering domain. The systematic literature review process is summarised, a number of reviews being undertaken by the authors and others are described and some lessons about the applicability of this practice to software engineering are extracted. The basic systematic literature review process seems appropriate to software engineering and the preparation and validation of a review protocol in advance of a review activity is especially valuable. The paper highlights areas where some adaptation of the process to accommodate the domain-specific characteristics of software engineering is needed as well as areas where improvements to current software engineering infrastructure and practices would enhance its applicability. In particular, infrastructure support provided by software engineering indexing databases is inadequate. Also, the quality of abstracts is poor; it is usually not possible to judge the relevance of a study from a review of the abstract alone.},
  keywords = {Empirical software engineering,Systematic literature review}
}

@article{christensenDynamicSamplingAutonomous2024,
  title = {Dynamic Sampling in Autonomous Process Optimization},
  author = {Christensen, Melodie and Xu, Yuting and Kwan, Eugene and Maso, Michael and Ji, Yining and Reibarkh, Mikhail and Sun, Alexandra and Liaw, Andy and Fier, Patrick and Grosser, Shane and Hein, Jason},
  year = {2024},
  month = apr,
  journal = {Chemical Science},
  volume = {15},
  doi = {10.1039/D3SC06884F}
}

@book{comtetAdvancedCombinatoricsArt1974,
  title = {Advanced Combinatorics : The Art of Finite and Infinite Expansions},
  author = {Comtet, {\relax Louis}.},
  year = {1974},
  edition = {Rev. and enl. ed.},
  publisher = {D. Reidel},
  address = {Dordrecht},
  isbn = {90-277-0380-9},
  keywords = {Combinatorial analysis}
}

@inproceedings{d.orescaninDataLakehouseNovel2021,
  title = {Data {{Lakehouse}} - a {{Novel Step}} in {{Analytics Architecture}}},
  booktitle = {2021 44th {{International Convention}} on {{Information}}, {{Communication}} and {{Electronic Technology}} ({{MIPRO}})},
  author = {{Ore{\v s}{\v c}anin} and {Hlupi{\'c}}},
  year = {2021},
  pages = {1242--1246},
  doi = {10.23919/MIPRO52101.2021.9597091},
  isbn = {2623-8764}
}

@incollection{DBLP:books/mk/gray93/Serlin93,
  title = {The History of {{DebitCredit}} and the {{TPC}}},
  booktitle = {The Benchmark Handbook for Database and Transaction Systems (2nd Edition)},
  author = {Serlin, Omri},
  editor = {Gray, Jim},
  year = {1993},
  publisher = {Morgan Kaufmann},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  ee = {db/books/collections/gray93/Serlin93.html},
  isbn = {1-55860-292-5}
}

@article{DBS-024,
  title = {The Design and Implementation of Modern Column-Oriented Database Systems},
  author = {Abadi, Daniel and Boncz, Peter and Harizopoulos, Stavros and Idreos, Stratos and Madden, Samuel},
  year = {2013},
  journal = {Foundations and Trends{\textregistered} in Databases},
  volume = {5},
  number = {3},
  pages = {197--280},
  issn = {1931-7883},
  doi = {10.1561/1900000024}
}

@inproceedings{dingInstanceOptimizedDataLayouts2021,
  type = {10.1145/3448016.3457270},
  title = {Instance-{{Optimized Data Layouts}} for {{Cloud Analytics Workloads}}},
  booktitle = {Proceedings of the 2021 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Ding, Jialin and Minhas, Umar Farooq and Chandramouli, Badrish and Wang, Chi and Li, Yinan and Li, Ying and Kossmann, Donald and Gehrke, Johannes and Kraska, Tim},
  year = {2021},
  pages = {418--431},
  publisher = {Association for Computing Machinery},
  address = {Virtual Event, China},
  isbn = {978-1-4503-8343-1},
  keywords = {cloud analytics instance-optimized databases},
  file = {/Users/david/Zotero/storage/RUTW6KPL/Ding et al. - 2021 - Instance-Optimized Data Layouts for Cloud Analytic.pdf}
}

@misc{dixonPentahoHadoopData2010,
  title = {Pentaho, {{Hadoop}}, and {{Data Lakes}}},
  author = {Dixon, James},
  year = {2010},
  month = oct,
  journal = {James Dixon's Blog},
  urldate = {2024-10-17},
  abstract = {Earlier this week, at Hadoop World in New York,~ Pentaho announced availability of our first Hadoop release. As part of the initial research into the Hadoop arena I talked to many companies that us{\dots}},
  langid = {english},
  file = {/Users/david/Zotero/storage/PCHRL3A9/pentaho-hadoop-and-data-lakes.html}
}

@book{doringForschungsmethodenUndEvaluation2016,
  title = {Forschungsmethoden Und {{Evaluation}} in Den {{Sozial-}} Und {{Humanwissenschaften}}},
  author = {D{\"o}ring, Nicola and Bortz, J{\"u}rgen},
  year = {2016},
  journal = {Springer-Lehrbuch},
  edition = {5., vollst. {\"u}berarb., akt. u. erw. Aufl. 2016},
  address = {Berlin, Heidelberg},
  isbn = {3642410898 (Sekund{\"a}rausgabe)},
  keywords = {Evaluation Forschungsmethoden Humanwissenschaften Sozialwissenschaften}
}

@misc{elshawi2019automatedmachinelearningstateoftheart,
  title = {Automated Machine Learning: {{State-of-the-art}} and Open Challenges},
  author = {Elshawi, Radwa and Maher, Mohamed and Sakr, Sherif},
  year = {2019},
  eprint = {1906.02287},
  primaryclass = {cs.LG},
  archiveprefix = {arXiv},
  file = {/Users/david/Zotero/storage/JDMU34TB/Elshawi et al. - 2019 - Automated Machine Learning State-of-The-Art and O.pdf}
}

@inproceedings{fetaiWorkloaddrivenAdaptiveData2015,
  title = {Workload-Driven Adaptive Data Partitioning and Distribution --- {{The Cumulus}} Approach},
  booktitle = {2015 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  author = {{Fetai} and {Murezzan} and {Schuldt}},
  year = {2015},
  pages = {1688--1697},
  doi = {10.1109/BigData.2015.7363940}
}

@article{Feurer_Springenberg_Hutter_2015,
  title = {Initializing Bayesian Hyperparameter Optimization via Meta-Learning},
  author = {Feurer, Matthias and Springenberg, Jost and Hutter, Frank},
  year = {2015},
  month = feb,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {29},
  number = {1},
  doi = {10.1609/aaai.v29i1.9354},
  annotation = {Abstract note: \&lt;p\&gt; Model selection and hyperparameter optimization is crucial in applying machine learning to a novel dataset. Recently, a subcommunity of machine learning has focused on solving this problem with Sequential Model-based Bayesian Optimization (SMBO), demonstrating substantial successes in many applications. However, for computationally expensive algorithms the overhead of hyperparameter optimization can still be prohibitive. In this paper we mimic a strategy human domain experts use: speed up optimization by starting from promising configurations that performed well on similar datasets. The resulting initialization technique integrates naturally into the generic SMBO framework and can be trivially applied to any SMBO method. To validate our approach, we perform extensive experiments with two established SMBO frameworks (Spearmint and SMAC) with complementary strengths; optimizing two machine learning frameworks on 57 datasets. Our initialization procedure yields mild improvements for low-dimensional hyperparameter optimization and substantially improves the state of the art for the more complex combined algorithm selection and hyperparameter optimization problem. \&lt;/p\&gt;}
}

@incollection{Feurer2019,
  title = {Hyperparameter Optimization},
  booktitle = {Automated Machine Learning: {{Methods}}, Systems, Challenges},
  author = {Feurer, Matthias and Hutter, Frank},
  editor = {Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin},
  year = {2019},
  pages = {3--33},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-05318-5_1},
  abstract = {Recent interest in complex and computationally expensive machine learning models with many hyperparameters, such as automated machine learning (AutoML) frameworks and deep neural networks, has resulted in a resurgence of research on hyperparameter optimization (HPO). In this chapter, we give an overview of the most prominent approaches for HPO. We first discuss blackbox function optimization methods based on model-free methods and Bayesian optimization. Since the high computational demand of many modern machine learning applications renders pure blackbox optimization extremely costly, we next focus on modern multi-fidelity methods that use (much) cheaper variants of the blackbox function to approximately assess the quality of hyperparameter settings. Lastly, we point to open problems and future research directions.},
  isbn = {978-3-030-05318-5},
  file = {/Users/david/Zotero/storage/C8QUZ989/Feurer and Hutter - 2019 - Hyperparameter Optimization.pdf}
}

@inproceedings{gaoLearningBitAllocations2024,
  type = {10.1145/3663742.3663975},
  title = {Learning {{Bit Allocations}} for {{Z-Order Layouts}} in {{Analytic Data Systems}}},
  booktitle = {Proceedings of the {{Seventh International Workshop}} on {{Exploiting Artificial Intelligence Techniques}} for {{Data Management}}},
  author = {Gao, Jenny and Ding, Jialin and Sudhir, Sivaprasad and Madden, Samuel},
  year = {2024},
  pages = {Article 5},
  publisher = {Association for Computing Machinery},
  address = {Santiago, AA, Chile},
  isbn = {9798400706806},
  file = {/Users/david/Zotero/storage/3LSX9LU4/Gao et al. - 2024 - Learning Bit Allocations for Z-Order Layouts in An.pdf}
}

@article{gorlaVerticalFragmentationDatabases2008,
  title = {Vertical {{Fragmentation}} in {{Databases Using Data-Mining Technique}}},
  author = {Gorla, Narasimhaiah and Betty, Pang W.Y.},
  year = {2008},
  journal = {International Journal of Data Warehousing and Mining (IJDWM)},
  volume = {4},
  number = {3},
  pages = {35--53},
  publisher = {IGI Global},
  address = {Hershey, PA, USA},
  issn = {1548-3924},
  doi = {10.4018/jdwm.2008070103},
  abstract = {A new approach to vertical fragmentation in relational databases is proposed using association rules, a data-mining technique. Vertical fragmentation can enhance the performance of database systems by reducing the number of disk accesses needed by transactions. By adapting Apriori algorithm, a design methodology for vertical partitioning is proposed. The heuristic methodology is tested using two real-life databases for various minimum support levels and minimum confidence levels. In the smaller database, the partitioning solution obtained matched the optimal solution using exhaustive enumeration. The application of our method on the larger database resulted in the partitioning solution that has an improvement of 41.05\% over unpartitioned solution and took less than a second to produce the solution. We provide future research directions on extending the procedure to distributed and object-oriented database designs.}
}

@article{gregorPositioningPresentingDesign2013,
  title = {Positioning and {{Presenting Design Science Research}} for {{Maximum Impact}}},
  author = {Gregor, Shirley and Hevner, Alan},
  year = {2013},
  journal = {MIS Quarterly},
  volume = {37},
  number = {2},
  eprint = {43825912},
  eprinttype = {jstor},
  pages = {337--355},
  publisher = {Management Information Systems Research Center, University of Minnesota},
  issn = {02767783},
  urldate = {2024-09-26},
  abstract = {Design science research (DSR) has staked its rightful ground as an important and legitimate Information Systems (IS) research paradigm. We contend that DSR has yet to attain its full potential impact on the development and use of information systems due to gaps in the understanding and application of DSR concepts and methods. This essay aims to help researchers (1) appreciate the levels of artifact abstractions that may be DSR contributions, (2) identify appropriate ways of consuming and producing knowledge when they are preparing journal articles or other scholarly works, (3) understand and position the knowledge contributions of their research projects, and (4) structure a DSR article so that it emphasizes significant contributions to the knowledge base. Our focal contribution is the DSR knowledge contribution framework with two dimensions based on the existing state of knowledge in both the problem and solution domains for the research opportunity under study. In addition, we propose a DSR communication schema with similarities to more conventional publication patterns, but which substitutes the description of the DSR artifact in place of a traditional results section. We evaluate the DSR contribution framework and the DSR communication schema via examinations of DSR exemplar publications.},
  keywords = {computer science discipline,design artifact,design science research (DSR),DSR theory,engineering discipline,information systems,knowledge,knowledge contribution framework,publication schema}
}

@article{guabtniWorkloaddrivenApproachDatabase2013,
  title = {A Workload-Driven Approach to Database Query Processing in the Cloud},
  author = {Guabtni, Adnene and Ranjan, Rajiv and Rabhi, Fethi A.},
  year = {2013},
  month = mar,
  journal = {The Journal of Supercomputing},
  volume = {63},
  number = {3},
  pages = {722--736},
  issn = {1573-0484},
  doi = {10.1007/s11227-011-0717-y},
  abstract = {This paper is concerned with data provisioning services (information search, retrieval, storage, etc.) dealing with a large and heterogeneous information repository. Increasingly, this class of services is being hosted and delivered through Cloud infrastructures. Although such systems are becoming popular, existing resource management methods (e.g. load-balancing techniques) do not consider workload patterns nor do they perform well when subjected to non-uniformly distributed datasets. If these problems can be solved, this class of services can be made to operate in more a scalable, efficient, and reliable manner.}
}

@inproceedings{Gupta2012AnER,
  title = {An Efficient Range Partitioning Method for Finding Frequent Patterns from Huge Database},
  author = {Gupta and {Satsangi} and Scholar, {\relax M.tech}.},
  year = {2012}
}

@incollection{hankinsDataMorphingAdaptive2003,
  title = {- {{Data Morphing}}: {{An Adaptive}}, {{Cache-Conscious Storage Technique}}},
  booktitle = {Proceedings 2003 {{VLDB Conference}}},
  author = {Hankins, Richard A. and Patel, Jignesh M.},
  editor = {Freytag, Johann-Christoph and Lockemann, Peter and Abiteboul, Serge and Carey, Michael and Selinger, Patricia and Heuer, Andreas},
  year = {2003},
  month = jan,
  pages = {417--428},
  publisher = {Morgan Kaufmann},
  address = {San Francisco},
  doi = {10.1016/B978-012722442-8/50044-6},
  abstract = {Publisher Summary This chapter proposes a flexible data storage technique called Data Morphing. Using Data Morphing, a cache-efficient attribute layout, called a partition, is first determined through an analysis of the query workload. This partition is then used as a template for storing data in a cache efficient way. This chapter presents two algorithms for computing partitions, and presents a versatile storage model that accommodates the dynamic reorganization of the attributes in a file. It experimentally demonstrates that the Data Morphing technique provides a significant performance improvement over both the traditional N-ary storage model and the PAX model. The number of processor cache misses has a critical impact on the performance of database management systems (DBMSs) running on servers with large main-memory configurations. In turn, the cache utilization of database systems is highly dependent on the physical organization of the records in main-memory.},
  isbn = {978-0-12-722442-8}
}

@inproceedings{hansertAmelioratingDataCompression2022,
  type = {10.1145/3530050.3532923},
  title = {Ameliorating Data Compression and Query Performance through Cracked {{Parquet}}},
  booktitle = {Proceedings of the {{International Workshop}} on {{Big Data}} in {{Emergent Distributed Environments}}},
  author = {Hansert, Patrick and Michel, Sebastian},
  year = {2022},
  pages = {Article 6},
  publisher = {Association for Computing Machinery},
  address = {Philadelphia, Pennsylvania},
  isbn = {978-1-4503-9346-1},
  keywords = {Dremel-encoding compression data skipping disaggregated systems partitioning}
}

@article{hansertPartitionDontSort2024,
  type = {10.14778/3681954.3682013},
  title = {Partition, {{Don}}'t {{Sort}}! {{Compression Boosters}} for {{Cloud Data Ingestion Pipelines}}},
  author = {Hansert, Patrick and Michel, Sebastian},
  year = {2024},
  journal = {Proc. VLDB Endow.},
  volume = {17},
  number = {11},
  pages = {3456--3469},
  publisher = {VLDB Endowment},
  isbn = {2150-8097}
}

@article{hevnerDesignScienceInformation2004,
  title = {Design {{Science}} in {{Information Systems Research}}},
  author = {Hevner, Alan and March, Salvatore and Park, Jinsoo and Ram, Sudha},
  year = {2004},
  journal = {MIS Quarterly},
  volume = {28},
  number = {1},
  pages = {75--105},
  publisher = {Management Information Systems Research Center, University of Minnesota},
  issn = {02767783},
  doi = {10.2307/25148625},
  urldate = {2024-11-06},
  abstract = {[Two paradigms characterize much of the research in the Information Systems discipline: behavioral science and design science. The behavioral-science paradigm seeks to develop and verify theories that explain or predict human or organizational behavior. The design-science paradigm seeks to extend the boundaries of human and organizational capabilities by creating new and innovative artifacts. Both paradigms are foundational to the IS discipline, positioned as it is at the confluence of people, organizations, and technology. Our objective is to describe the performance of design-science research in Information Systems via a concise conceptual framework and clear guidelines for understanding, executing, and evaluating the research. In the design-science paradigm, knowledge and understanding of a problem domain and its solution are achieved in the building and application of the designed artifact. Three recent exemplars in the research literature are used to demonstrate the application of these guidelines. We conclude with an analysis of the challenges of performing high-quality design-science research in the context of the broader IS community.]}
}

@incollection{hevnerDesignScienceResearch2010,
  title = {Design {{Science Research}} in {{Information Systems}}},
  booktitle = {Design {{Research}} in {{Information Systems}}: {{Theory}} and {{Practice}}},
  author = {Hevner, Alan and Chatterjee, Samir},
  editor = {Hevner, Alan and Chatterjee, Samir},
  year = {2010},
  pages = {9--22},
  publisher = {Springer US},
  address = {Boston, MA},
  doi = {10.1007/978-1-4419-5653-8_2},
  abstract = {Design activities are central to most applied disciplines. Research in design has a long history in many fields including architecture, engineering, education, psychology, and the fine arts (Cross 2001). The computing and information technology (CIT) field since its advent in the late 1940s has appropriated many of the ideas, concepts, and methods of design science that have originated in these other disciplines. However, information systems (IS) as composed of inherently mutable and adaptable hardware, software, and human interfaces provide many unique and challenging design problems that call for new and creative ideas.},
  isbn = {978-1-4419-5653-8},
  file = {/Users/david/Zotero/storage/HPAGEAXQ/Hevner and Chatterjee - 2010 - Design Science Research in Information Systems.pdf}
}

@article{hevnerThreeCycleView2007,
  title = {A {{Three Cycle View}} of {{Design Science Research}}},
  author = {Hevner, Alan},
  year = {2007},
  month = jan,
  journal = {Scandinavian Journal of Information Systems},
  volume = {19}
}

@book{inmonDataLakeArchitecture2016,
  title = {Data {{Lake Architecture}}: {{Designing}} the {{Data Lake}} and {{Avoiding}} the {{Garbage Dump}}},
  author = {Inmon, Bill},
  year = {2016},
  publisher = {Technics Publications, LLC},
  isbn = {1-63462-117-4}
}

@article{ivanovImpactColumnarFile2019,
  title = {The Impact of Columnar File Formats on {{SQL}}-on-hadoop Engine Performance: {{A}} Study on {{ORC}} and {{Parquet}}},
  author = {Ivanov, Todor and Pergolesi, Matteo},
  year = {2019},
  month = sep,
  journal = {Concurrency and Computation: Practice and Experience},
  volume = {32},
  doi = {10.1002/cpe.5523}
}

@inproceedings{jamiesonNonstochasticBestArm2016,
  title = {Non-Stochastic {{Best Arm Identification}} and {{Hyperparameter Optimization}}},
  booktitle = {Proceedings of the 19th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Jamieson, Kevin and Talwalkar, Ameet},
  editor = {Gretton, Arthur and Robert, Christian C.},
  year = {2016},
  volume = {51},
  pages = {240--248},
  publisher = {PMLR},
  address = {Proceedings of Machine Learning Research},
  abstract = {Motivated by the task of hyperparameter optimization, we introduce the {\textbackslash}em non-stochastic best-arm identification problem. We identify an attractive algorithm  for this setting that makes no assumptions on the convergence behavior of the arms' losses, has no free-parameters to adjust, provably outperforms the uniform allocation baseline in favorable conditions, and performs comparably (up to {\textbackslash}log factors) otherwise. Next, by leveraging the iterative nature of many  learning algorithms, we cast hyperparameter optimization as an instance of non-stochastic best-arm identification. Our empirical results show that, by allocating more resources to promising hyperparameter settings, our approach achieves comparable test accuracies an order of magnitude faster than the uniform strategy. The robustness and simplicity of our approach makes it well-suited to ultimately replace the uniform strategy currently used in most machine learning software packages.},
  file = {/Users/david/Zotero/storage/2ARPUQFI/Jamieson and Talwalkar - Non-stochastic Best Arm Identiﬁcation and Hyperpar.pdf}
}

@inproceedings{kangJigsawDataStorage2021,
  type = {10.1145/3448016.3457547},
  title = {Jigsaw: {{A Data Storage}} and {{Query Processing Engine}} for {{Irregular Table Partitioning}}},
  booktitle = {Proceedings of the 2021 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Kang, Donghe and Jiang, Ruochen and Blanas, Spyros},
  year = {2021},
  pages = {898--911},
  publisher = {Association for Computing Machinery},
  address = {Virtual Event, China},
  isbn = {978-1-4503-8343-1}
}

@techreport{kitchenhamGuidelinesPerformingSystematic2007,
  title = {Guidelines for Performing {{Systematic Literature Reviews}} in {{Software Engineering}}},
  author = {Kitchenham, Barbara Ann and Charters, Stuart},
  year = {2007},
  number = {EBSE 2007-001},
  abstract = {The objective of this report is to propose comprehensive guidelines for systematic literature reviews appropriate for software engineering researchers, including PhD students. A systematic literature review is a means of evaluating and interpreting all available research relevant to a particular research question, topic area, or phenomenon of interest. Systematic reviews aim to present a fair evaluation of a research topic by using a trustworthy, rigorous, and auditable methodology. The guidelines presented in this report were derived from three existing guidelines used by medical researchers, two books produced by researchers with social science backgrounds and discussions with researchers from other disciplines who are involved in evidence-based practice. The guidelines have been adapted to reflect the specific problems of software engineering research. The guidelines cover three phases of a systematic literature review: planning the review, conducting the review and reporting the review. They provide a relatively high level description. They do not consider the impact of the research questions on the review procedures, nor do they specify in detail the mechanisms needed to perform meta-analysis.},
  keywords = {engineering evidence evidence-based literature real review software systematic}
}

@book{Lee2024-kg,
  title = {Delta {{Lake}}: {{The}} Definitive Guide: {{Modern}} Data Lakehouse Architectures with Data Lakes},
  author = {Lee, Denny and Babu, Prashanth and Venkata, Prashanth Babu Velanati and Wentling, Tristen and Haines, Scott},
  year = {2024},
  month = nov,
  abstract = {Discover how Delta Lake simplifies the process of building data lakehouses and data pipelines at scale. With this practical guide, data engineers, data scientists, and data analysts will explore key data reliability challenges and learn to apply modern data engineering and management techniques. You'll also understand how ACID transactions bring reliability to data lakehouses at scale. This book helps you: Understand key data reliability challenges Examine data management and engineering techniques using the modern data stack Realize data reliability improvements using Delta Lake Concurrently run streaming and batch jobs against your data lake Execute update, delete, and merge commands Use time travel to rollback and examine previous versions of your data Build a streaming data quality pipeline following the medallion construct},
  langid = {english}
}

@article{leeZSKYEfficientSkyline2010,
  title = {Z-{{SKY}}: An Efficient Skyline Query Processing Framework Based on {{Z-order}}},
  author = {Lee, Ken C. K. and Lee, Wang-Chien and Zheng, Baihua and Li, Huajing and Tian, Yuan},
  year = {2010},
  month = jun,
  journal = {The VLDB Journal},
  volume = {19},
  number = {3},
  pages = {333--362},
  issn = {0949-877X},
  doi = {10.1007/s00778-009-0166-x},
  abstract = {Given a set of data points in a multidimensional space, a skyline query retrieves those data points that are not dominated by any other point in the same dataset. Observing that the properties of Z-order space filling curves (or Z-order curves) perfectly match with the dominance relationships among data points in a geometrical data space, we, in this paper, develop and present a novel and efficient processing framework to evaluate skyline queries and their variants, and to support skyline result updates based on Z-order curves. This framework consists of ZBtree, i.e., an index structure to organize a source dataset and skyline candidates, and a suite of algorithms, namely, (1) ZSearch, which processes skyline queries, (2) ZInsert, ZDelete and ZUpdate, which incrementally maintain skyline results in presence of source dataset updates, (3) ZBand, which answers skyband queries, (4) ZRank, which returns top-ranked skyline points, (5) k-ZSearch, which evaluates k-dominant skyline queries, and (6) ZSubspace, which supports skyline queries on a subset of dimensions. While derived upon coherent ideas and concepts, our approaches are shown to outperform the state-of-the-art algorithms that are specialized to address particular skyline problems, especially when a large number of skyline points are resulted, via comprehensive experiments.},
  file = {/Users/david/Zotero/storage/YTDXXE3P/Lee et al. - 2010 - Z-SKY an efficient skyline query processing frame.pdf}
}

@book{liPluggableLearnedIndex2021,
  title = {A {{Pluggable Learned Index Method}} via {{Sampling}} and {{Gap Insertion}}},
  author = {Li and Chen, Daoyuan and Ding, Bolin and Zeng, Kai and Zhou, Jingren},
  year = {2021},
  month = jan,
  doi = {10.48550/arXiv.2101.00808}
}

@article{liuEnhancingStorageEfficiency2024,
  title = {Enhancing {{Storage Efficiency}} and {{Performance}}: {{A Survey}} of {{Data Partitioning Techniques}}},
  author = {Liu and Li and Chen},
  year = {2024},
  month = mar,
  journal = {Journal of Computer Science and Technology},
  volume = {39},
  number = {2},
  pages = {346--368},
  issn = {1860-4749},
  doi = {10.1007/s11390-024-3538-1},
  abstract = {Data partitioning techniques are pivotal for optimal data placement across storage devices, thereby enhancing resource utilization and overall system throughput. However, the design of effective partition schemes faces multiple challenges, including considerations of the cluster environment, storage device characteristics, optimization objectives, and the balance between partition quality and computational efficiency. Furthermore, dynamic environments necessitate robust partition detection mechanisms. This paper presents a comprehensive survey structured around partition deployment environments, outlining the distinguishing features and applicability of various partitioning strategies while delving into how these challenges are addressed. We discuss partitioning features pertaining to database schema, table data, workload, and runtime metrics. We then delve into the partition generation process, segmenting it into initialization and optimization stages. A comparative analysis of partition generation and update algorithms is provided, emphasizing their suitability for different scenarios and optimization objectives. Additionally, we illustrate the applications of partitioning in prevalent database products and suggest potential future research directions and solutions. This survey aims to foster the implementation, deployment, and updating of high-quality partitions for specific system scenarios.}
}

@article{mathisDataLakes2017,
  title = {Data {{Lakes}}},
  author = {Mathis, Christian},
  year = {2017},
  month = nov,
  journal = {Datenbank-Spektrum},
  volume = {17},
  number = {3},
  pages = {289--293},
  issn = {1610-1995},
  doi = {10.1007/s13222-017-0272-7},
  abstract = {By moving data into a centralized, scalable storage location inside an organization -- the data lake -- companies and other institutions aim to discover new information and to generate value from the data. The data lake can help to overcome organizational boundaries and system complexity. However, to generate value from the data, additional techniques, tools, and processes need to be established which help to overcome data integration and other challenges around this approach. Although there is a certain agreed-on notion of the central idea, there is no accepted definition what components or functionality a data lake has or how an architecture looks like. Throughout this article, we will start with the central idea and discuss various related aspects and technologies.}
}

@misc{mazumdarDataLakehouseData2023,
  title = {The {{Data Lakehouse}}: {{Data Warehousing}} and {{More}}},
  shorttitle = {The {{Data Lakehouse}}},
  author = {Mazumdar, Dipankar and Hughes, Jason and Onofre, J. B.},
  year = {2023},
  month = oct,
  number = {arXiv:2310.08697},
  eprint = {2310.08697},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-07-25},
  abstract = {Relational Database Management Systems designed for Online Analytical Processing (RDBMS-OLAP) have been foundational to democratizing data and enabling analytical use cases such as business intelligence and reporting for many years. However, RDBMS-OLAP systems present some well-known challenges. They are primarily optimized only for relational workloads, lead to proliferation of data copies which can become unmanageable, and since the data is stored in proprietary formats, it can lead to vendor lock-in, restricting access to engines, tools, and capabilities beyond what the vendor offers. As the demand for data-driven decision making surges, the need for a more robust data architecture to address these challenges becomes ever more critical. Cloud data lakes have addressed some of the shortcomings of RDBMS-OLAP systems, but they present their own set of challenges. More recently, organizations have often followed a two-tier architectural approach to take advantage of both these platforms, leveraging both cloud data lakes and RDBMSOLAP systems. However, this approach brings additional challenges, complexities, and overhead.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Databases},
  file = {/Users/david/Zotero/storage/SNNWS7QE/Mazumdar et al. - 2023 - The Data Lakehouse Data Warehousing and More.pdf}
}

@inproceedings{mockusBayesianMethodsSeeking1974,
  title = {On {{Bayesian Methods}} for {{Seeking}} the {{Extremum}}},
  booktitle = {Proceedings of the {{IFIP Technical Conference}}},
  author = {Mockus, Jonas},
  year = {1974},
  pages = {400--404},
  publisher = {Springer-Verlag},
  isbn = {3-540-07165-2},
  file = {/Users/david/Zotero/storage/8RRKW9IU/Mockus - 1974 - On Bayesian Methods for Seeking the Extremum.pdf}
}

@inproceedings{nambiarMakingTPCDS2006,
  title = {The Making of {{TPC-DS}}},
  booktitle = {Proceedings of the 32nd International Conference on {{Very}} Large Data Bases},
  author = {Nambiar, Raghunath Othayoth and Poess, Meikel},
  year = {2006},
  pages = {1049--1058},
  publisher = {VLDB Endowment},
  address = {Seoul, Korea},
  file = {/Users/david/Zotero/storage/LTCWQRTG/the_making_of_tpcds.pdf}
}

@book{nambiarPerformanceEvaluationBenchmarking2019,
  title = {Performance {{Evaluation}} and {{Benchmarking}} for the {{Era}} of {{Cloud}}(s): 11th {{TPC Technology Conference}}, {{TPCTC}} 2019, {{Los Angeles}}, {{CA}}, {{USA}}, {{August}} 26, 2019, {{Revised Selected Papers}}},
  editor = {Nambiar, Raghunath and Poess, Meikel},
  year = {2019},
  publisher = {Springer-Verlag},
  address = {Los Angeles, CA, USA},
  isbn = {978-3-030-55023-3}
}

@inproceedings{nathanLearningMultiDimensionalIndexes2020,
  type = {10.1145/3318464.3380579},
  title = {Learning {{Multi-Dimensional Indexes}}},
  booktitle = {Proceedings of the 2020 {{ACM SIGMOD International Conference}} on {{Management}} of {{Data}}},
  author = {Nathan, Vikram and Ding, Jialin and Alizadeh, Mohammad and Kraska, Tim},
  year = {2020},
  pages = {985--1000},
  publisher = {Association for Computing Machinery},
  address = {Portland, OR, USA},
  isbn = {978-1-4503-6735-6},
  keywords = {databases in-memory indexing multi-dimensional primary index},
  file = {/Users/david/Zotero/storage/NPYZ6GUT/Nathan et al. - 2020 - Learning Multi-Dimensional Indexes.pdf}
}

@misc{NewFeaturesApache2021,
  title = {New Features from {{Apache Hudi}} 0.7.0 and 0.8.0 Available on {{Amazon EMR}} {\textbar} {{AWS Big Data Blog}}},
  year = {2021},
  month = dec,
  urldate = {2024-08-07},
  chapter = {Amazon EMR},
  howpublished = {https://aws.amazon.com/blogs/big-data/new-features-from-apache-hudi-0-7-0-and-0-8-0-available-on-amazon-emr/},
  langid = {american}
}

@inproceedings{NIPS2011_86e8f7ab,
  title = {Algorithms for Hyperparameter Optimization},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Bergstra and Bardenet and Bengio and K{\'e}gl},
  editor = {{Shawe-Taylor}, J. and Zemel, R. and Bartlett, P. and Pereira, F. and Weinberger, K.Q.},
  year = {2011},
  volume = {24},
  publisher = {Curran Associates, Inc.},
  file = {/Users/david/Zotero/storage/W2UAG3SH/Bergstra et al. - 2011 - Algorithms for Hyper-Parameter Optimization.pdf}
}

@article{nunamakerSystemsDevelopmentInformation1990,
  title = {Systems {{Development}} in {{Information Systems Research}}},
  author = {Nunamaker and Chen and Purdin},
  year = {1990},
  journal = {Journal of Management Information Systems},
  volume = {7},
  number = {3},
  eprint = {40397957},
  eprinttype = {jstor},
  pages = {89--106},
  publisher = {Taylor \& Francis, Ltd.},
  issn = {07421222},
  urldate = {2024-11-06},
  abstract = {[In this paper, the use of systems development as a methodology in information systems (IS) research is described and defended. A framework to explain the nature of systems development as a research methodology in IS research is proposed. Use of this methodology in the engineering field in general is compared with its use specifically in computer science and computer engineering. An integrated program for conducting IS research that incorporates theory building, systems development, experimentation, and observation is proposed. Progress in several application domains is reviewed to provide a basis upon which to argue that systems development is a valid research methodology. A systems development research process is presented from a methodological perspective. Software engineering, which is the basic method of applying the systems development research methodology, is then discussed. It is the authors' belief that systems development and other research methodologies are complementary and that an integrated multi-dimensional and multimethodological approach will generate fruitful IS research results. The premise is that research contributions can result from systems development, experimentation, observation, and performance testing of the systems under development and that all of these research approaches are needed to investigate different aspects of the research question.]}
}

@inproceedings{pmlr-v80-falkner18a,
  title = {{{BOHB}}: {{Robust}} and Efficient Hyperparameter Optimization at Scale},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  author = {Falkner, Stefan and Klein, Aaron and Hutter, Frank},
  editor = {Dy, Jennifer and Krause, Andreas},
  year = {2018-07-10/2018-07-15},
  series = {Proceedings of Machine Learning Research},
  volume = {80},
  pages = {1437--1446},
  publisher = {PMLR},
  abstract = {Modern deep learning methods are very sensitive to many hyperparameters, and, due to the long training times of state-of-the-art models, vanilla Bayesian hyperparameter optimization is typically computationally infeasible. On the other hand, bandit-based configuration evaluation approaches based on random search lack guidance and do not converge to the best configurations as quickly. Here, we propose to combine the benefits of both Bayesian optimization and bandit-based methods, in order to achieve the best of both worlds: strong anytime performance and fast convergence to optimal configurations. We propose a new practical state-of-the-art hyperparameter optimization method, which consistently outperforms both Bayesian optimization and Hyperband on a wide range of problem types, including high-dimensional toy functions, support vector machines, feed-forward neural networks, Bayesian neural networks, deep reinforcement learning, and convolutional neural networks. Our method is robust and versatile, while at the same time being conceptually simple and easy to implement.},
  file = {/Users/david/Zotero/storage/M82C3J6I/Falkner et al. - 2018 - BOHB Robust and Efficient Hyperparameter Optimiza.pdf;/Users/david/Zotero/storage/X7VHF89Z/Falkner et al. - 2018 - BOHB Robust and Efficient Hyperparameter Optimiza.pdf}
}

@article{poessNewTPCBenchmarks2000,
  title = {New {{TPC}} Benchmarks for Decision Support and Web Commerce},
  author = {Poess, Meikel and Floyd, Chris},
  year = {2000},
  month = dec,
  journal = {Sigmod Record},
  volume = {29},
  number = {4},
  pages = {64--71},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  issn = {0163-5808},
  doi = {10.1145/369275.369291},
  abstract = {For as long as there have been DBMS's and applications that use them, there has been interest in the performance characteristics that these systems exhibit. This month's column describes some of the recent work that has taken place in TPC, the Transaction Processing Performance Council.TPC-A and TPC-B are obsolete benchmarks that you might have heard about in the past. TPC-C V3.5 is the current benchmark for OLTP systems. Introduced in 1992, it has been run on many hardware platforms and DBMS's. Indeed, the TPC web site currently lists 202 TPC-C benchmark results. Due to its maturity, TPC-C will not be discussed in this article.We've asked two very knowledgeable individuals to write this article. Meikel Poess is the chair of the TPC H and TPC-R Subcommittees and Chris Floyd is the chair of the TPC-W Subcommittee. We greatly appreciate their efforts.A wealth of information can be found at the TPC web site [ 1 ]. This information includes the benchmark specifications themselves, TPC membership information, and benchmark results.},
  issue_date = {Dec. 2000}
}

@incollection{poessTPCDS2018,
  title = {{{TPC-DS}}},
  booktitle = {Encyclopedia of {{Big Data Technologies}}},
  author = {Poess, Meikel},
  editor = {Sakr, Sherif and Zomaya, Albert},
  year = {2018},
  pages = {1--8},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-63962-8_127-1},
  isbn = {978-3-319-63962-8}
}

@inproceedings{pourabbasMinimizingIndexSize2012,
  title = {Minimizing {{Index Size}} by {{Reordering Rows}} and {{Columns}}},
  booktitle = {Scientific and {{Statistical Database Management}}},
  author = {Pourabbas, Elaheh and Shoshani, Arie and Wu, Kesheng},
  editor = {Ailamaki, Anastasia and Bowers, Shawn},
  year = {2012},
  pages = {467--484},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  abstract = {Sizes of compressed bitmap indexes and compressed data are significantly affected by the order of data records. The optimal orders of rows and columns that minimizes the index sizes is known to be NP-hard to compute. Instead of seeking the precise global optimal ordering, we develop accurate statistical formulas that compute approximate solutions. Since the widely used bitmap indexes are compressed with variants of the run-length encoding (RLE) method, our work concentrates on computing the sizes of bitmap indexes compressed with the basic Run-Length Encoding. The resulting formulas could be used for choosing indexes to build and to use. In this paper, we use the formulas to develop strategies for reordering rows and columns of a data table. We present empirical measurements to show that our formulas are accurate for a wide range of data. Our analysis confirms that the heuristics of sorting columns with low column cardinalities first is indeed effective in reducing the index sizes. We extend the strategy by showing that columns with the same cardinality should be ordered from high skewness to low skewness.},
  isbn = {978-3-642-31235-9}
}

@book{powerDecisionSupportSystems2002,
  title = {Decision {{Support Systems}}: {{Concepts}} and {{Resources}} for {{Managers}}},
  author = {Power, Daniel},
  year = {2002},
  month = mar,
  isbn = {1-56720-497-X}
}

@inproceedings{quamarSWORDScalableWorkloadaware2013,
  type = {10.1145/2452376.2452427},
  title = {{{SWORD}}: Scalable Workload-Aware Data Placement for Transactional Workloads},
  booktitle = {Proceedings of the 16th {{International Conference}} on {{Extending Database Technology}}},
  author = {Quamar, Abdul and Kumar, K. Ashwin and Deshpande, Amol},
  year = {2013},
  pages = {430--441},
  publisher = {Association for Computing Machinery},
  address = {Genoa, Italy},
  isbn = {978-1-4503-1597-5},
  keywords = {data placement database-as-a-service graph partitioning quorums replication}
}

@article{reySeamlessIntegrationParquet2023,
  title = {Seamless {{Integration}} of {{Parquet Files}} into {{Data Processing}}},
  author = {Rey, Alice and Freitag, Michael and Neumann, Thomas},
  editor = {{K{\"o}nig-Ries}, Stefanie and Lehner, Wolfgang and Vossen, Gottfried and Birgitta, Scherzinger},
  year = {2023},
  journal = {BTW 2023},
  publisher = {Gesellschaft f{\"u}r Informatik e.V.},
  issn = {978-3-88579-725-8},
  doi = {10.18420/BTW2023-12},
  abstract = {Relational database systems are still the most powerful tool for data analysis. However, the steps necessary to bring existing data into the database make them unattractive for data exploration, especially when the data is stored in data lakes where users often use Parquet files, a binary column-oriented file format.This paper presents a fast Parquet framework that tackles these problems without costly ETL steps. We incrementally collect information during query execution.We create statistics that enhance future queries. In addition, we split the file into chunks for which we store the data ranges. We call these synopses. They allow us to skip entire sections in future queries.We show that these techniques only add minor overhead to the first query and are of benefit for future requests.Our evaluation demonstrates that our implementation can achieve comparable results to database relations and that we can outperform existing systems by up to an order of magnitude.},
  langid = {english}
}

@misc{rongDynamicDataLayout2024,
  title = {Dynamic Data Layout Optimization with Worst-Case Guarantees},
  author = {Rong, Kexin and Liu, Paul and Sonje, Sarah Ashok and Charikar, Moses},
  year = {2024},
  eprint = {2405.04984},
  primaryclass = {cs.DB},
  archiveprefix = {arXiv}
}

@article{sawant2013database,
  title = {Database Partitioning: {{A}} Review Paper},
  author = {Sawant, Mayur and Kinage, Kishor and Pilankar, Pooja and Chaudhari, Nikhil},
  year = {2013},
  journal = {International Journal of Innovative Technology and Exploring Engineering},
  volume = {3},
  number = {5}
}

@online{seidlFileFormat,
  author       = {Seidl, Ed},
  title        = {File Format},
  organization = {Apache Parquet},
  url          = {https://parquet.apache.org/docs/file-format/},
  urldate      = {2024-10-30},
  langid       = {english},
}

@inproceedings{snoekPracticalBayesianOptimization2012,
  title = {Practical {{Bayesian}} Optimization of Machine Learning Algorithms},
  booktitle = {Proceedings of the 25th {{International Conference}} on {{Neural Information Processing Systems}} - {{Volume}} 2},
  author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P.},
  year = {2012},
  pages = {2951--2959},
  publisher = {Curran Associates Inc.},
  address = {Lake Tahoe, Nevada},
  file = {/Users/david/Zotero/storage/S2XYATTH/Snoek et al. - 2012 - Practical Bayesian Optimization of Machine Learnin.pdf}
}

@article{strongSelfSortingMapEfficient2014,
  title = {Self-{{Sorting Map}}: {{An Efficient Algorithm}} for {{Presenting Multimedia Data}} in {{Structured Layouts}}},
  author = {{Strong} and {Gong}},
  year = {2014},
  month = jun,
  journal = {IEEE Transactions on Multimedia},
  volume = {16},
  number = {4},
  pages = {1045--1058},
  issn = {1941-0077},
  doi = {10.1109/TMM.2014.2306183}
}

@article{sudhirPandoEnhancedData2023,
  type = {10.14778/3598581.3598601},
  title = {Pando: {{Enhanced Data Skipping}} with {{Logical Data Partitioning}}},
  author = {Sudhir, Sivaprasad and Tao, Wenbo and Laptev, Nikolay and Habis, Cyrille and Cafarella, Michael and Madden, Samuel},
  year = {2023},
  journal = {Proc. VLDB Endow.},
  volume = {16},
  number = {9},
  pages = {2316--2329},
  publisher = {VLDB Endowment},
  isbn = {2150-8097},
  file = {/Users/david/Zotero/storage/Y3Q25PG4/Sudhir et al. - 2023 - Pando Enhanced Data Skipping with Logical Data Pa.pdf}
}

@article{sudhirReplicatedLayoutInmemory2021,
  type = {10.14778/3503585.3503606},
  title = {Replicated Layout for In-Memory Database Systems},
  author = {Sudhir, Sivaprasad and Cafarella, Michael and Madden, Samuel},
  year = {2021},
  journal = {Proc. VLDB Endow.},
  volume = {15},
  number = {4},
  pages = {984--997},
  publisher = {VLDB Endowment},
  isbn = {2150-8097},
  file = {/Users/david/Zotero/storage/NPQVAXA2/Sudhir et al. - 2021 - Replicated layout for in-memory database systems.pdf}
}

@misc{sugiura2023zorderedrangerefinementmultidimensional,
  title = {Z-Ordered Range Refinement for Multi-Dimensional Range Queries},
  author = {Sugiura, Kento and Ishikawa, Yoshiharu},
  year = {2023},
  eprint = {2305.12732},
  primaryclass = {cs.DB},
  archiveprefix = {arXiv}
}

@phdthesis{sunSkippingorientedDataDesign2017,
  title = {Skipping-Oriented {{Data Design}} for {{Large-Scale Analytics}}},
  author = {Sun, Liwen},
  year = {2017},
  address = {Berkeley, CA},
  abstract = {As data volumes continue to expand, analytics approaches that require exhaustively scanning data sets become untenable. For this reason, modern analytics systems employ data skipping techniques to avoid looking at large volumes of irrelevant data. By maintaining some metadata for each block of data, a query may skip a data block if the metadata indicates that the block does not contain relevant data. The e ectiveness of data skipping, however, depends on how the underlying data are organized into blocks. In this dissertation, we propose a  ne-grained data layout framework, called ``Generalized Skipping-Oriented Partitioning and Replication'' (GSOP-R), which aims to maximize query performance through aggressive data skipping. Based on observations of real-world analytics workloads, we  nd that the workload patterns can be summarized as a succinct set of features. The GSOP-R framework uses these features to transform the incoming data into a small set of feature vectors, and then performs clustering algorithms using the feature vectors instead of the actual data. A resulting GSOP-R layout scheme is highly  exible. For instance, it allows di erent columns to be horizontally partitioned in di erent ways and supports replication of only parts of rows or columns. We developed several designs for GSOP-R on Apache Spark and Apache Parquet and then evaluated their performance using two public benchmarks and several real-world workloads. Our results show that GSOP-R can reduce the amount of data scanned and improve end-to-end query response times over the state-of-the-art techniques by a factor of 2 to 9.},
  langid = {english},
  school = {University of California at Berkeley},
  file = {/Users/david/Zotero/storage/FVC75H2C/Sun - Skipping-oriented Data Design for Large-Scale Anal.pdf}
}

@article{sunSkippingorientedPartitioningColumnar2016,
  type = {10.14778/3025111.3025123},
  title = {Skipping-Oriented Partitioning for Columnar Layouts},
  author = {Sun, Liwen and Franklin, Michael J. and Wang, Jiannan and Wu, Eugene},
  year = {2016},
  journal = {Proc. VLDB Endow.},
  volume = {10},
  number = {4},
  pages = {421--432},
  publisher = {VLDB Endowment},
  isbn = {2150-8097},
  file = {/Users/david/Zotero/storage/GTVZFGGS/Sun et al. - 2016 - Skipping-oriented partitioning for columnar layout.pdf}
}

@phdthesis{swerskyImprovingBayesianOptimization2017,
  title = {Improving {{Bayesian Optimization}} for {{Machine Learning}} Using {{Expert Priors}}},
  author = {Swersky, Kevin},
  year = {2017},
  month = jun,
  address = {Toronto},
  abstract = {Deep neural networks have recently become astonishingly successful at many machine learning problems such as object recognition and speech recognition, and they are now also being used in many new and creative ways. However, their performance critically relies on the proper setting of numerous hyperparameters. Manual tuning by an expert researcher has been a traditionally effective approach, however it is becoming increasingly infeasible as models become more complex and machine learning systems become further embedded within larger automated systems. Bayesian optimization has recently been proposed as a strategy for intelligently optimizing the hyperparameters of deep neural networks and other machine learning systems; it has been shown in many cases to outperform experts, and provides a promising way to reduce both the computational and human time required. Regardless, expert researchers can still be quite effective at hyperparameter tuning due to their ability to incorporate contextual knowledge and intuition into their search, while traditional Bayesian optimization treats each problem as a black box and therefore cannot take advantage of this knowledge. In this thesis, we draw inspiration from these abilities and incorporate them into the Bayesian optimization framework as additional prior information. These extensions include the ability to transfer knowledge between problems, the ability to transform the problem domain into one that is easier to optimize, and the ability to terminate experiments when they are no longer deemed to be promising, without requiring their training to converge. We demonstrate in experiments across a range of machine learning models that these extensions significantly reduce the cost and increase the robustness of Bayesian optimization for automatic hyperparameter tuning.},
  langid = {english},
  school = {University of Toronto},
  file = {/Users/david/Zotero/storage/FB8N77HD/Swersky - Improving Bayesian Optimization for Machine Learni.pdf}
}

@article{sym13020195,
  title = {Choosing a Data Storage Format in the Apache Hadoop System Based on Experimental Evaluation Using Apache Spark},
  author = {Belov, Vladimir and Tatarintsev, Andrey and Nikulchev, Evgeny},
  year = {2021},
  journal = {Symmetry},
  volume = {13},
  number = {195},
  issn = {2073-8994},
  doi = {10.3390/sym13020195},
  abstract = {One of the most important tasks of any platform for big data processing is storing the data received. Different systems have different requirements for the storage formats of big data, which raises the problem of choosing the optimal data storage format to solve the current problem. This paper describes the five most popular formats for storing big data, presents an experimental evaluation of these formats and a methodology for choosing the format. The following data storage formats will be considered: avro, CSV, JSON, ORC, parquet. At the first stage, a comparative analysis of the main characteristics of the studied formats was carried out; at the second stage, an experimental evaluation of these formats was prepared and carried out. For the experiment, an experimental stand was deployed with tools for processing big data installed on it. The aim of the experiment was to find out characteristics of data storage formats, such as the volume and processing speed for different operations using the Apache Spark framework. In addition, within the study, an algorithm for choosing the optimal format from the presented alternatives was developed using tropical optimization methods. The result of the study is presented in the form of a technique for obtaining a vector of ratings of data storage formats for the Apache Hadoop system, based on an experimental assessment using Apache Spark.}
}

@book{vaismanDataWarehouseSystems2014,
  title = {Data {{Warehouse Systems}}: {{Design}} and {{Implementation}}},
  author = {Vaisman, Alejandro and Zimnyi, Esteban},
  year = {2014},
  publisher = {Springer Publishing Company, Incorporated},
  isbn = {3-642-54654-4}
}

@inproceedings{varadarajanDBDesignerCustomizablePhysical2014,
  title = {DBDesigner: {A} Customizable Physical Design Tool for {Vertica} Analytic Database},
  booktitle = {2014 {{IEEE}} 30th International Conference on Data Engineering},
  author = {{Varadarajan} and {Bharathan} and {Cary} and {Dave} and {Bodagala}},
  year = {2014},
  pages = {1084--1095},
  doi = {10.1109/ICDE.2014.6816725},
  isbn = {2375-026X},
  file = {/Users/david/Zotero/storage/X4WDD6GL/R. Varadarajan et al. - 2014 - DBDesigner A customizable physical design tool fo.pdf}
}

@misc{watanabeTreeStructuredParzenEstimator2023,
  title = {Tree-{{Structured Parzen Estimator}}: {{Understanding Its Algorithm Components}} and {{Their Roles}} for {{Better Empirical Performance}}},
  shorttitle = {Tree-{{Structured Parzen Estimator}}},
  author = {Watanabe, Shuhei},
  year = {2023},
  month = may,
  number = {arXiv:2304.11127},
  eprint = {2304.11127},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-06-26},
  abstract = {Recent advances in many domains require more and more complicated experiment design. Such complicated experiments often have many parameters, which necessitate parameter tuning. Tree-structured Parzen estimator (TPE), a Bayesian optimization method, is widely used in recent parameter tuning frameworks. Despite its popularity, the roles of each control parameter and the algorithm intuition have not been discussed so far. In this tutorial, we will identify the roles of each control parameter and their impacts on hyperparameter optimization using a diverse set of benchmarks. We compare our recommended setting drawn from the ablation study with baseline methods and demonstrate that our recommended setting improves the performance of TPE. Our TPE implementation is available at https://github.com/nabenabe0928/tpe/tree/single-opt.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/david/Zotero/storage/UYDHDQYT/Watanabe - 2023 - Tree-Structured Parzen Estimator Understanding It.pdf}
}

@article{websterAnalyzingPrepareFuture2002,
  title = {Analyzing the {{Past}} to {{Prepare}} for the {{Future}}: {{Writing}} a {{Literature Review}}},
  author = {Webster and Watson},
  year = {2002},
  month = jun,
  journal = {MIS Quarterly},
  volume = {26},
  doi = {10.2307/4132319},
  file = {/Users/david/Zotero/storage/BTLYN8MA/Webster and Watson - 2002 - Analyzing the Past to Prepare for the Future Writ.pdf}
}

@article{weintraubOptimizingCloudData2024,
  title = {Optimizing {{Cloud Data Lake Queries With}} a {{Balanced Coverage Plan}}},
  author = {{Weintraub} and {E. Gudes} and {S. Dolev} and {J. D. Ullman}},
  year = {Jan.-March 2024},
  journal = {IEEE Transactions on Cloud Computing},
  volume = {12},
  number = {1},
  pages = {84--99},
  issn = {2168-7161},
  doi = {10.1109/TCC.2023.3339208},
  file = {/Users/david/Zotero/storage/UUCM4UUB/G. Weintraub et al. - 2024 - Optimizing Cloud Data Lake Queries With a Balanced.pdf}
}

@article{wuWuFastbitEfficient2005,
  title = {Wu, {{K}}.: {{Fastbit}}: An Efficient Indexing Technology for Accelerating Data-Intensive Science. {{Journal}} of {{Physics}}: {{Conference Series}} 16, 556},
  author = {Wu, Kesheng},
  year = {2005},
  month = jan,
  journal = {J. Phys.: Conf. Ser.},
  volume = {16},
  pages = {556},
  doi = {10.1088/1742-6596/16/1/077}
}

@article{xiaoGuidanceConductingSystematic2019,
  type = {10.1177/{{0739456X17723971}}},
  title = {Guidance on {{Conducting}} a {{Systematic Literature Review}}},
  author = {Xiao and Watson},
  year = {2019},
  journal = {Journal of Planning Education and Research},
  volume = {39},
  number = {1},
  pages = {93--112},
  abstract = {Literature reviews establish the foundation of academic inquires. However, in the planning field, we lack rigorous systematic reviews. In this article, through a systematic search on the methodology of literature review, we categorize a typology of literature reviews, discuss steps in conducting a systematic literature review, and provide suggestions on how to enhance rigor in literature reviews in planning education and research.},
  keywords = {literature reviewmethodologysynthesistypology}
}

@inproceedings{yangQdtreeLearningData2020,
  type = {10.1145/3318464.3389770},
  title = {Qd-Tree: {{Learning Data Layouts}} for {{Big Data Analytics}}},
  booktitle = {Proceedings of the 2020 {{ACM SIGMOD International Conference}} on {{Management}} of {{Data}}},
  author = {Yang, Zongheng and Chandramouli, Badrish and Wang, Chi and Gehrke, Johannes and Li, Yinan and Minhas, Umar Farooq and Larson, Per-{\AA}ke and Kossmann, Donald and Acharya, Rajeev},
  year = {2020},
  pages = {193--208},
  publisher = {Association for Computing Machinery},
  address = {Portland, OR, USA},
  isbn = {978-1-4503-6735-6},
  keywords = {storage query processing indexing deep reinforcement learning deep learning data partitioning data layout data analytics big data OLAP},
  file = {/Users/david/Zotero/storage/E28955H3/Yang et al. - 2020 - Qd-tree Learning Data Layouts for Big Data Analyt.pdf}
}
